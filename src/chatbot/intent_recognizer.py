"""
Intent Recognition and Query Expansion module.
Provides semantic understanding of user queries and intelligent query rewriting.
"""

from typing import Dict, List, Tuple, Optional
import numpy as np
from sentence_transformers import SentenceTransformer, util
from loguru import logger
import re
import simplemma

from config.settings import settings


class IntentRecognizer:
    """
    BUSINESS_RULE: Semantic intent recognition and query expansion.
    Transforms vague user queries into precise search queries using embeddings.
    """
    
    def __init__(self, embedding_model: SentenceTransformer):
        """
        Initialize intent recognizer.
        
        Args:
            embedding_model: Pre-loaded sentence transformer model
        """
        self.embedding_model = embedding_model
        
        # Initialize Bulgarian lemmatization (simplemma)
        try:
            # simplemma 1.1.2 doesn't have setup() method
            logger.info("Bulgarian lemmatization initialized (simplemma)")
        except Exception as e:
            logger.warning(f"Failed to initialize Bulgarian lemmatization: {e}")
        
        # Canonical queries for different intents
        self.canonical_queries = {
            "payment": [
                "–ö–∞–∫–≤–∏ —Å–∞ –Ω–∞—á–∏–Ω–∏—Ç–µ –Ω–∞ –ø–ª–∞—â–∞–Ω–µ?",
                "–ú–µ—Ç–æ–¥–∏ –∑–∞ –ø–ª–∞—â–∞–Ω–µ",
                "–ö–∞–∫ –º–æ–≥–∞ –¥–∞ –ø–ª–∞—Ç—è?",
                "–û–ø—Ü–∏–∏ –∑–∞ –ø–ª–∞—â–∞–Ω–µ",
                "–ü–ª–∞—â–∞–Ω–µ —Å –∫–∞—Ä—Ç–∞",
                "–ë–∞–Ω–∫–æ–≤ –ø—Ä–µ–≤–æ–¥",
                "–ù–∞—á–∏–Ω–∏ –Ω–∞ –ø–ª–∞—â–∞–Ω–µ",
                "–ö–∞–∫ –¥–∞ –ø–ª–∞—Ç—è",
                "–ü–ª–∞—â–∞–Ω–µ –Ω–∞ –ø–æ—Ä—ä—á–∫–∞"
            ],
            "delivery": [
                "–ö–æ–ª–∫–æ –≤—Ä–µ–º–µ –æ—Ç–Ω–µ–º–∞ –¥–æ—Å—Ç–∞–≤–∫–∞—Ç–∞?",
                "–í—Ä–µ–º–µ –∑–∞ –¥–æ—Å—Ç–∞–≤–∫–∞",
                "–ö–æ–≥–∞ —â–µ –ø—Ä–∏—Å—Ç–∏–≥–Ω–µ –ø–æ—Ä—ä—á–∫–∞—Ç–∞?",
                "–°—Ä–æ–∫–æ–≤–µ –∑–∞ –¥–æ—Å—Ç–∞–≤–∫–∞",
                "–ï–∫—Å–ø—Ä–µ—Å–Ω–∞ –¥–æ—Å—Ç–∞–≤–∫–∞",
                "–î–æ—Å—Ç–∞–≤–∫–∞ –¥–æ –∞–¥—Ä–µ—Å"
            ],
            "return_policy": [
                "–ö–∞–∫ —Å–µ –≤—Ä—ä—â–∞ –ø—Ä–æ–¥—É–∫—Ç?",
                "–ü–æ–ª–∏—Ç–∏–∫–∞ –∑–∞ –≤—Ä—ä—â–∞–Ω–µ",
                "–†–µ–∫–ª–∞–º–∞—Ü–∏–∏",
                "–í—Ä—ä—â–∞–Ω–µ –Ω–∞ —Å—Ç–æ–∫–∏",
                "–ì–∞—Ä–∞–Ω—Ü–∏—è –∑–∞ –≤—Ä—ä—â–∞–Ω–µ",
                "–£—Å–ª–æ–≤–∏—è –∑–∞ –≤—Ä—ä—â–∞–Ω–µ"
            ],
            "warranty": [
                "–ö–∞–∫–≤–∞ –µ –≥–∞—Ä–∞–Ω—Ü–∏—è—Ç–∞?",
                "–ì–∞—Ä–∞–Ω—Ü–∏—è –∑–∞ –ø—Ä–æ–¥—É–∫—Ç–∏",
                "–°–µ—Ä–≤–∏–∑ –∏ –ø–æ–¥–¥—Ä—ä–∂–∫–∞",
                "–ì–∞—Ä–∞–Ω—Ü–∏–æ–Ω–µ–Ω —Å—Ä–æ–∫",
                "–†–µ–º–æ–Ω—Ç –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–∏"
            ],
            "shipping": [
                "–î–æ—Å—Ç–∞–≤–∫–∞ –≤ —á—É–∂–±–∏–Ω–∞",
                "–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–∞ –¥–æ—Å—Ç–∞–≤–∫–∞",
                "–î–æ—Å—Ç–∞–≤–∫–∞ –≤ –ï–≤—Ä–æ–ø–∞",
                "–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–∏ —Ä–∞–∑—Ö–æ–¥–∏",
                "–ö—É—Ä–∏–µ—Ä—Å–∫–∞ –¥–æ—Å—Ç–∞–≤–∫–∞"
            ],
            "contact": [
                "–ö–∞–∫ –¥–∞ —Å–µ —Å–≤—ä—Ä–∂–∞ —Å –≤–∞—Å?",
                "–ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
                "–¢–µ–ª–µ—Ñ–æ–Ω –∑–∞ –≤—Ä—ä–∑–∫–∞",
                "Email –∑–∞ –ø–æ–¥–¥—Ä—ä–∂–∫–∞",
                "–ê–¥—Ä–µ—Å –Ω–∞ –º–∞–≥–∞–∑–∏–Ω–∞"
            ],
            "working_hours": [
                "–†–∞–±–æ—Ç–∏—Ç–µ –ª–∏ –≤ –Ω–µ–¥–µ–ª—è?",
                "–ö–æ–≥–∞ —Ä–∞–±–æ—Ç–∏—Ç–µ?",
                "–†–∞–±–æ—Ç–Ω–æ –≤—Ä–µ–º–µ",
                "–û—Ç–≤–æ—Ä–µ–Ω–∏ –ª–∏ —Å—Ç–µ –≤ —Å—ä–±–æ—Ç–∞?",
                "–î–æ –∫–æ–ª–∫–æ —á–∞—Å–∞ —Ä–∞–±–æ—Ç–∏—Ç–µ?",
                "–û—Ç –∫–æ–ª–∫–æ —á–∞—Å–∞ –æ—Ç–≤–∞—Ä—è—Ç–µ?",
                "–†–∞–±–æ—Ç–∏—Ç–µ –ª–∏ –≤ –ø–æ—á–∏–≤–Ω–∏ –¥–Ω–∏?",
                "–ì—Ä–∞—Ñ–∏–∫ –Ω–∞ —Ä–∞–±–æ—Ç–∞"
            ],
            "products": [
                "–ö–∞–∫–≤–∏ –ø—Ä–æ–¥—É–∫—Ç–∏ –∏–º–∞—Ç–µ?",
                "–ü—Ä–µ–¥–ª–∞–≥–∞—Ç–µ –ª–∏ –ª–∞–ø—Ç–æ–ø–∏?",
                "–ò–º–∞—Ç–µ –ª–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–∏?",
                "–ü—Ä–æ–¥–∞–≤–∞—Ç–µ –ª–∏ —á–∞—Å–æ–≤–Ω–∏—Ü–∏?",
                "–ö–∞–∫–≤–æ –∏–º–∞—Ç–µ –≤ –∫–∞—Ç–∞–ª–æ–≥–∞?",
                "–ü–æ–∫–∞–∂–µ—Ç–µ –º–∏ –ø—Ä–æ–¥—É–∫—Ç–∏—Ç–µ",
                "–ò–º–∞—Ç–µ –ª–∏ —Ç–∞–±–ª–µ—Ç–∏?",
                "–ü—Ä–µ–¥–ª–∞–≥–∞—Ç–µ –ª–∏ –∫–æ–º–ø—é—Ç—Ä–∏?",
                "–ö–∞–∫–≤–∏ —Å—Ç–æ–∫–∏ –∏–º–∞—Ç–µ?",
                "–ö–∞—Ç–∞–ª–æ–≥ —Å –ø—Ä–æ–¥—É–∫—Ç–∏",
                "–ù–∞–ª–∏—á–Ω–∏ —Å—Ç–æ–∫–∏",
                "–ù–æ–≤–∏ –ø—Ä–æ–¥—É–∫—Ç–∏",
                "–ü–æ–ø—É–ª—è—Ä–Ω–∏ –ø—Ä–æ–¥—É–∫—Ç–∏"
            ],
            "promotions": [
                "–ò–º–∞—Ç–µ –ª–∏ –ø—Ä–æ–º–æ—Ü–∏–∏?",
                "–ö–∞–∫–≤–∏ —Å–∞ –∞–∫—Ü–∏–∏—Ç–µ?",
                "–ò–º–∞—Ç–µ –ª–∏ –ø—Ä–æ–º–æ –∫–æ–¥?",
                "–ò–º–∞ –ª–∏ –æ—Ç—Å—Ç—ä–ø–∫–∏?",
                "–ò–º–∞ –ª–∏ –Ω–∞–º–∞–ª–µ–Ω–∏—è?",
                "–ò–º–∞—Ç–µ –ª–∏ –≤–∞—É—á–µ—Ä–∏?",
                "–ò–º–∞ –ª–∏ —Å–ø–µ—Ü–∏–∞–ª–Ω–∏ –æ—Ñ–µ—Ä—Ç–∏?",
                "–ü—Ä–æ–º–æ—Ü–∏–æ–Ω–∞–ª–Ω–∏ –∫–æ–¥–æ–≤–µ",
                "–û—Ç—Å—Ç—ä–ø–∫–∏ –∑–∞ —Å—Ç—É–¥–µ–Ω—Ç–∏",
                "–ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∏ –æ—Ç—Å—Ç—ä–ø–∫–∏"
            ]
        }
        
        # Context boosters for semantic enhancement
        self.context_boosters = {
            "payment": "–Ω–∞—á–∏–Ω–∏ –Ω–∞ –ø–ª–∞—â–∞–Ω–µ, –ø–ª–∞—â–∞–Ω–µ –æ–Ω–ª–∞–π–Ω, –Ω–∞–ª–æ–∂–µ–Ω –ø–ª–∞—Ç–µ–∂, –∫–∞—Ä—Ç–∞, –±–∞–Ω–∫–æ–≤ –ø—Ä–µ–≤–æ–¥",
            "delivery": "–¥–æ—Å—Ç–∞–≤–∫–∞, –∫—É—Ä–∏–µ—Ä, –≤—Ä–µ–º–µ –∑–∞ –¥–æ—Å—Ç–∞–≤–∫–∞, —Å—Ä–æ–∫, –¥–æ—Å—Ç–∞–≤–∫–∞ –¥–æ –∞–¥—Ä–µ—Å",
            "return_policy": "–≤—Ä—ä—â–∞–Ω–µ –Ω–∞ —Å—Ç–æ–∫–∏, —Ä–µ–∫–ª–∞–º–∞—Ü–∏–∏, –≥–∞—Ä–∞–Ω—Ü–∏—è, —É—Å–ª–æ–≤–∏—è –∑–∞ –≤—Ä—ä—â–∞–Ω–µ",
            "warranty": "–≥–∞—Ä–∞–Ω—Ü–∏—è, —Å–µ—Ä–≤–∏–∑, —Ä–µ–º–æ–Ω—Ç, –ø–æ–¥–¥—Ä—ä–∂–∫–∞, –≥–∞—Ä–∞–Ω—Ü–∏–æ–Ω–µ–Ω —Å—Ä–æ–∫",
            "shipping": "–¥–æ—Å—Ç–∞–≤–∫–∞, –∫—É—Ä–∏–µ—Ä, —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç, —Ä–∞–∑—Ö–æ–¥–∏ –∑–∞ –¥–æ—Å—Ç–∞–≤–∫–∞",
            "contact": "—Ç–µ–ª–µ—Ñ–æ–Ω, email, –∞–¥—Ä–µ—Å, –≤—Ä—ä–∑–∫–∞, –ø–æ–¥–¥—Ä—ä–∂–∫–∞",
            "products": "–∫–∞—Ç–∞–ª–æ–≥, –ø—Ä–æ–¥—É–∫—Ç–∏, —Å—Ç–æ–∫–∏, –∞—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç, –Ω–∞–ª–∏—á–Ω–∏",
            "promotions": "–ø—Ä–æ–º–æ—Ü–∏–∏, –∞–∫—Ü–∏–∏, –æ—Ç—Å—Ç—ä–ø–∫–∏, –Ω–∞–º–∞–ª–µ–Ω–∏—è, –ø—Ä–æ–º–æ –∫–æ–¥, –≤–∞—É—á–µ—Ä–∏, –æ—Ñ–µ—Ä—Ç–∏",
            "working_hours": "—Ä–∞–±–æ—Ç–Ω–æ –≤—Ä–µ–º–µ, –≥—Ä–∞—Ñ–∏–∫, –ø–æ–Ω–µ–¥–µ–ª–Ω–∏–∫, –≤—Ç–æ—Ä–Ω–∏–∫, —Å—Ä—è–¥–∞, —á–µ—Ç–≤—ä—Ä—Ç—ä–∫, –ø–µ—Ç—ä–∫, —Å—ä–±–æ—Ç–∞, –Ω–µ–¥–µ–ª—è, –ø–æ—á–∏–≤–Ω–∏ –¥–Ω–∏"
        }
        
        # Canonical aliases for query normalization (expanded with conversational forms)
        self.aliases = {
            # Payment aliases
            "–∫–∞–∫ —Å–µ –ø–ª–∞—â–∞": "–Ω–∞—á–∏–Ω–∏ –Ω–∞ –ø–ª–∞—â–∞–Ω–µ",
            "–∫–∞–∫ –¥–∞ –ø–ª–∞—Ç—è": "–Ω–∞—á–∏–Ω–∏ –Ω–∞ –ø–ª–∞—â–∞–Ω–µ",
            "–∫–∞–∫ –¥–∞ –ø–ª–∞—â–∞–º": "–Ω–∞—á–∏–Ω–∏ –Ω–∞ –ø–ª–∞—â–∞–Ω–µ",
            "–Ω–∞—á–∏–Ω–∏ –Ω–∞ –ø–ª–∞—â–∞–Ω–µ": "–Ω–∞—á–∏–Ω–∏ –Ω–∞ –ø–ª–∞—â–∞–Ω–µ", 
            "–ø–ª–∞—â–∞–Ω–µ": "–Ω–∞—á–∏–Ω–∏ –Ω–∞ –ø–ª–∞—â–∞–Ω–µ",
            "–ø–ª–∞—â–∞": "–Ω–∞—á–∏–Ω–∏ –Ω–∞ –ø–ª–∞—â–∞–Ω–µ",
            "–ø–ª–∞—â–∞–º": "–Ω–∞—á–∏–Ω–∏ –Ω–∞ –ø–ª–∞—â–∞–Ω–µ",
            "—Ä–∞–∑–ø–ª–∞—â–∞–Ω–µ": "–Ω–∞—á–∏–Ω–∏ –Ω–∞ –ø–ª–∞—â–∞–Ω–µ",
            "–ø—Ä–∏–µ–º–∞—Ç–µ –ª–∏ –∫–∞—Ä—Ç–∞": "–Ω–∞—á–∏–Ω–∏ –Ω–∞ –ø–ª–∞—â–∞–Ω–µ",
            "–ø–ª–∞—â–∞–Ω–µ —Å –∫–∞—Ä—Ç–∞": "–Ω–∞—á–∏–Ω–∏ –Ω–∞ –ø–ª–∞—â–∞–Ω–µ",
            "–±–∞–Ω–∫–æ–≤ –ø—Ä–µ–≤–æ–¥": "–Ω–∞—á–∏–Ω–∏ –Ω–∞ –ø–ª–∞—â–∞–Ω–µ",
            
            # Delivery aliases
            "–¥–æ—Å—Ç–∞–≤–∫–∞": "–≤—Ä–µ–º–µ –∑–∞ –¥–æ—Å—Ç–∞–≤–∫–∞",
            "–∫–æ–≥–∞ –ø—Ä–∏—Å—Ç–∏–≥–∞": "–≤—Ä–µ–º–µ –∑–∞ –¥–æ—Å—Ç–∞–≤–∫–∞",
            "–∫–æ–≥–∞ —â–µ –¥–æ–π–¥–µ": "–≤—Ä–µ–º–µ –∑–∞ –¥–æ—Å—Ç–∞–≤–∫–∞",
            "–∫–æ–≥–∞ —â–µ –ø—Ä–∏—Å—Ç–∏–≥–Ω–µ": "–≤—Ä–µ–º–µ –∑–∞ –¥–æ—Å—Ç–∞–≤–∫–∞",
            "—Å—Ä–æ–∫ –∑–∞ –¥–æ—Å—Ç–∞–≤–∫–∞": "–≤—Ä–µ–º–µ –∑–∞ –¥–æ—Å—Ç–∞–≤–∫–∞",
            "–≤—Ä–µ–º–µ –∑–∞ –¥–æ—Å—Ç–∞–≤–∫–∞": "–≤—Ä–µ–º–µ –∑–∞ –¥–æ—Å—Ç–∞–≤–∫–∞",
            "–∫–æ–ª–∫–æ –≤—Ä–µ–º–µ –æ—Ç–Ω–µ–º–∞": "–≤—Ä–µ–º–µ –∑–∞ –¥–æ—Å—Ç–∞–≤–∫–∞",
            "–∫–æ–≥–∞ —â–µ –¥–æ–π–¥–µ –ø—Ä–∞—Ç–∫–∞—Ç–∞": "–≤—Ä–µ–º–µ –∑–∞ –¥–æ—Å—Ç–∞–≤–∫–∞",
            
            # Return aliases
            "–≤—Ä—ä—â–∞–Ω–µ": "–ø–æ–ª–∏—Ç–∏–∫–∞ –∑–∞ –≤—Ä—ä—â–∞–Ω–µ",
            "–≤—ä—Ä–Ω–∞": "–ø–æ–ª–∏—Ç–∏–∫–∞ –∑–∞ –≤—Ä—ä—â–∞–Ω–µ",
            "–∫–∞–∫ –¥–∞ –≤—ä—Ä–Ω–∞": "–ø–æ–ª–∏—Ç–∏–∫–∞ –∑–∞ –≤—Ä—ä—â–∞–Ω–µ",
            "–∏—Å–∫–∞–º –¥–∞ –≤—ä—Ä–Ω–∞": "–ø–æ–ª–∏—Ç–∏–∫–∞ –∑–∞ –≤—Ä—ä—â–∞–Ω–µ",
            "—Ä–µ–∫–ª–∞–º–∞—Ü–∏—è": "–ø–æ–ª–∏—Ç–∏–∫–∞ –∑–∞ –≤—Ä—ä—â–∞–Ω–µ",
            
            # Products aliases
            "–∫–∞–∫–≤–∏ –ø—Ä–æ–¥—É–∫—Ç–∏ –∏–º–∞—Ç–µ": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–∫–∞–∫–≤–æ –ø—Ä–æ–¥–∞–≤–∞—Ç–µ": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏", 
            "–∏–º–∞—Ç–µ –ª–∏ –ø—Ä–æ–¥—É–∫—Ç–∏": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–ø—Ä–µ–¥–ª–∞–≥–∞—Ç–µ –ª–∏": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–∏–º–∞—Ç–µ –ª–∏ —Å—Ç–æ–∫–∏": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–∫–∞—Ç–∞–ª–æ–≥": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–ø—Ä–æ–¥—É–∫—Ç–∏": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "—Å—Ç–æ–∫–∏": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–∞—Å–∞–æ—Ä—Ç–∏–º–µ–Ω—Ç": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–∏–º–∞ –ª–∏": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–Ω–∞–ª–∏—á–Ω–æ—Å—Ç": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–Ω–∞–ª–∏—á–µ–Ω": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–≤ –Ω–∞–ª–∏—á–Ω–æ—Å—Ç": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–∫—É–ø—è": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–ø–æ—Ä—ä—á–∞–º": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–≤–∑–µ–º–µ—Ç–µ –ª–∏": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–ø–æ–ª–∏—Ç–∏–∫–∞ –∑–∞ –≤—Ä—ä—â–∞–Ω–µ": "–ø–æ–ª–∏—Ç–∏–∫–∞ –∑–∞ –≤—Ä—ä—â–∞–Ω–µ",
            "–≤—ä—Ä–Ω–∞ –ø—Ä–æ–¥—É–∫—Ç": "–ø–æ–ª–∏—Ç–∏–∫–∞ –∑–∞ –≤—Ä—ä—â–∞–Ω–µ",
            "–≤—ä—Ä–Ω–∞ —Å—Ç–æ–∫–∞": "–ø–æ–ª–∏—Ç–∏–∫–∞ –∑–∞ –≤—Ä—ä—â–∞–Ω–µ",
            
            # Warranty aliases
            "–≥–∞—Ä–∞–Ω—Ü–∏—è": "–≥–∞—Ä–∞–Ω—Ü–∏—è –∑–∞ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–∏–º–∞ –ª–∏ –≥–∞—Ä–∞–Ω—Ü–∏—è": "–≥–∞—Ä–∞–Ω—Ü–∏—è –∑–∞ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–≥–∞—Ä–∞–Ω—Ü–∏—è –Ω–∞ —Å—Ç–æ–∫–∞—Ç–∞": "–≥–∞—Ä–∞–Ω—Ü–∏—è –∑–∞ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "—Å–µ—Ä–≤–∏–∑": "–≥–∞—Ä–∞–Ω—Ü–∏—è –∑–∞ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "—Ä–µ–º–æ–Ω—Ç": "–≥–∞—Ä–∞–Ω—Ü–∏—è –∑–∞ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–≥–∞—Ä–∞–Ω—Ü–∏—è –∑–∞ –ø—Ä–æ–¥—É–∫—Ç–∏": "–≥–∞—Ä–∞–Ω—Ü–∏—è –∑–∞ –ø—Ä–æ–¥—É–∫—Ç–∏",
            
            # Contact aliases
            "–∫–æ–Ω—Ç–∞–∫—Ç": "–∫–æ–Ω—Ç–∞–∫—Ç–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
            "–∫–æ–Ω—Ç–∞–∫—Ç–∏": "–∫–æ–Ω—Ç–∞–∫—Ç–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
            "—Ç–µ–ª–µ—Ñ–æ–Ω –∑–∞ –≤—Ä—ä–∑–∫–∞": "–∫–æ–Ω—Ç–∞–∫—Ç–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
            "—Ç–µ–ª–µ—Ñ–æ–Ω–µ–Ω –Ω–æ–º–µ—Ä": "–∫–æ–Ω—Ç–∞–∫—Ç–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
            "–∞–¥—Ä–µ—Å": "–∫–æ–Ω—Ç–∞–∫—Ç–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
            "–∫–æ–Ω—Ç–∞–∫—Ç–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è": "–∫–æ–Ω—Ç–∞–∫—Ç–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
            
            # Working hours aliases
            "—Ä–∞–±–æ—Ç–Ω–æ –≤—Ä–µ–º–µ": "–≥—Ä–∞—Ñ–∏–∫ –Ω–∞ —Ä–∞–±–æ—Ç–∞",
            "—Ä–∞–±–æ—Ç–∏—Ç–µ –ª–∏": "–≥—Ä–∞—Ñ–∏–∫ –Ω–∞ —Ä–∞–±–æ—Ç–∞",
            "–æ—Ç–≤–æ—Ä–µ–Ω–∏ –ª–∏ —Å—Ç–µ": "–≥—Ä–∞—Ñ–∏–∫ –Ω–∞ —Ä–∞–±–æ—Ç–∞",
            "–∫–æ–≥–∞ —Ä–∞–±–æ—Ç–∏—Ç–µ": "–≥—Ä–∞—Ñ–∏–∫ –Ω–∞ —Ä–∞–±–æ—Ç–∞",
            "–¥–æ –∫–æ–ª–∫–æ —á–∞—Å–∞": "–≥—Ä–∞—Ñ–∏–∫ –Ω–∞ —Ä–∞–±–æ—Ç–∞",
            "–æ—Ç –∫–æ–ª–∫–æ —á–∞—Å–∞": "–≥—Ä–∞—Ñ–∏–∫ –Ω–∞ —Ä–∞–±–æ—Ç–∞",
            "—Ä–∞–±–æ—Ç–∏—Ç–µ –ª–∏ –≤ –Ω–µ–¥–µ–ª—è": "–≥—Ä–∞—Ñ–∏–∫ –Ω–∞ —Ä–∞–±–æ—Ç–∞",
            "—Ä–∞–±–æ—Ç–∏—Ç–µ –ª–∏ –≤ —Å—ä–±–æ—Ç–∞": "–≥—Ä–∞—Ñ–∏–∫ –Ω–∞ —Ä–∞–±–æ—Ç–∞",
            "–ø–æ—á–∏–≤–Ω–∏ –¥–Ω–∏": "–≥—Ä–∞—Ñ–∏–∫ –Ω–∞ —Ä–∞–±–æ—Ç–∞",
            
            # Product aliases (—Ç–µ–ª–µ—Ñ–æ–Ω –∫–∞—Ç–æ –ø—Ä–æ–¥—É–∫—Ç)
            "—Ç–µ–ª–µ—Ñ–æ–Ω": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "—Å–º–∞—Ä—Ç—Ñ–æ–Ω": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏", 
            "–º–æ–±–∏–ª–µ–Ω": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "gsm": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            
            # Products aliases
            "–ø—Ä–æ–¥—É–∫—Ç–∏": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–∫–∞–∫–≤–∏ –ø—Ä–æ–¥—É–∫—Ç–∏": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–∫–∞—Ç–∞–ª–æ–≥": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏",
            "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏": "–∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–¥—É–∫—Ç–∏"
        }
        
        # Synonyms for query expansion (expanded for better coverage)
        self.synonyms = {
            "–ø–ª–∞—â–∞": ["–ø–ª–∞—â–∞–Ω–µ", "—Ä–∞–∑–ø–ª–∞—â–∞–Ω–µ", "–ø–∞—Ä–∏", "—Ü–µ–Ω–∞", "—Ç–∞–∫—Å–∞", "–∫–∞—Ä—Ç–∞", "–ø—Ä–µ–≤–æ–¥", "–Ω–∞—á–∏–Ω"],
            "–¥–æ—Å—Ç–∞–≤–∫–∞": ["–¥–æ—Å—Ç–∞–≤—è–Ω–µ", "—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç", "–∫—É—Ä–∏–µ—Ä", "–∏–∑–ø—Ä–∞—â–∞–Ω–µ", "–ø—Ä–∞—Ç–∫–∞", "–ø–∞–∫–µ—Ç", "—Å—Ä–æ–∫"],
            "–≤—Ä—ä—â–∞": ["—Ä–µ–∫–ª–∞–º–∞—Ü–∏—è", "–≤—ä–∑—Å—Ç–∞–Ω–æ–≤—è–≤–∞–Ω–µ", "–æ—Ç–º—è–Ω–∞", "–æ—Ç–∫–∞–∑", "–≤—ä—Ä–Ω–∞", "–ø–æ–ª–∏—Ç–∏–∫–∞"],
            "–≥–∞—Ä–∞–Ω—Ü–∏—è": ["—Å–µ—Ä–≤–∏–∑", "—Ä–µ–º–æ–Ω—Ç", "–ø–æ–¥–¥—Ä—ä–∂–∫–∞", "–æ–±—Å–ª—É–∂–≤–∞–Ω–µ", "–ø–æ–∫—Ä–∏—Ç–∏–µ", "–∑–∞—â–∏—Ç–∞"],
            "–≤—Ä–µ–º–µ": ["—Å—Ä–æ–∫", "–ø–µ—Ä–∏–æ–¥", "–¥–Ω–∏", "—á–∞—Å–æ–≤–µ", "–º–∏–Ω—É—Ç–∏", "–∫–æ–ª–∫–æ", "–∫–æ–≥–∞"],
            "–∫–∞–∫": ["–ø–æ –∫–∞–∫—ä–≤ –Ω–∞—á–∏–Ω", "–∫–∞–∫–≤–æ", "–∑–∞—â–æ", "–∫–æ–≥–∞", "–∫—ä–¥–µ", "–Ω–∞—á–∏–Ω", "–º–µ—Ç–æ–¥"],
            "–ø—Ä–æ–¥—É–∫—Ç": ["—Å—Ç–æ–∫–∏", "–∞—Ä—Ç–∏–∫—É–ª–∏", "–ø—Ä–µ–¥–º–µ—Ç–∏", "–Ω–µ—â–∞", "–∫–∞—Ç–∞–ª–æ–≥", "–∞—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç"],
            "–∫–æ–Ω—Ç–∞–∫—Ç": ["—Ç–µ–ª–µ—Ñ–æ–Ω", "–∞–¥—Ä–µ—Å", "–∏–º–µ–π–ª", "–≤—Ä—ä–∑–∫–∞", "—Å–≤—ä—Ä–∑–≤–∞–Ω–µ", "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"]
        }
        
        # Precompute embeddings for canonical queries (performance optimization)
        self.intent_embeddings = {}
        for intent, examples in self.canonical_queries.items():
            self.intent_embeddings[intent] = self.embedding_model.encode(examples)
        
        logger.success("‚úì Canonical intent embeddings precomputed")
        logger.info("IntentRecognizer initialized with semantic understanding")
    
    def _auto_correct_bulgarian(self, text: str) -> str:
        """
        BUSINESS_RULE: Light auto-correction of common Bulgarian spelling mistakes
        before regex/intent analysis.
        
        Args:
            text: Query text to correct
            
        Returns:
            Auto-corrected text
        """
        corrections = {
            "—Ä–µ–¥–ª–∞–≥–∞—Ç–µ": "–ø—Ä–µ–¥–ª–∞–≥–∞—Ç–µ",
            "—Ä–µ–¥–ª–∞–≥–∞—Ç–µ –ª–∏": "–ø—Ä–µ–¥–ª–∞–≥–∞—Ç–µ –ª–∏",
            "–≤–∑–µ–º–µ—Ç–µ –ª–∏": "–≤–∑–µ–º–∞—Ç–µ –ª–∏",
            "–∞—Å–∞–æ—Ä—Ç–∏–º–µ–Ω—Ç": "–∞—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç",
            "–∏–º–∞ –ª–∏ –Ω–∞–ª–∏—á–µ–Ω": "–∏–º–∞—Ç–µ –ª–∏ –Ω–∞–ª–∏—á–µ–Ω",
            "–∏–º–∞ –ª–∏ –≤ –Ω–∞–ª–∏—á–Ω–æ—Å—Ç": "–∏–º–∞—Ç–µ –ª–∏ –≤ –Ω–∞–ª–∏—á–Ω–æ—Å—Ç",
            "–Ω–∞–ª–∏—á–Ω–∏ –ª–∏": "–Ω–∞–ª–∏—á–Ω–∏ –ª–∏ —Å–∞",
            "–∏–º–∞–π—Ç–µ": "–∏–º–∞—Ç–µ",
            "–ø—Ä–æ–¥–∞–≤–∞—Ç–µ –ª–∏": "–ø—Ä–æ–¥–∞–≤–∞—Ç–µ –ª–∏",
        }

        text_fixed = text
        for wrong, right in corrections.items():
            text_fixed = re.sub(rf"\b{wrong}\b", right, text_fixed, flags=re.IGNORECASE)

        # Light punctuation normalization
        text_fixed = re.sub(r"\s+", " ", text_fixed).strip()
        if text_fixed != text:
            logger.debug(f"Auto-corrected spelling: '{text}' ‚Üí '{text_fixed}'")

        return text_fixed
    
    def _lemmatize_bulgarian(self, text: str) -> str:
        """
        BUSINESS_RULE: Lemmatize Bulgarian text using simplemma.
        
        Args:
            text: Text to lemmatize
            
        Returns:
            Lemmatized text
        """
        try:
            # Tokenize text into words first
            words = text.split()
            lemmatized_words = [simplemma.lemmatize(word, lang="bg") for word in words]
            return " ".join(lemmatized_words)
        except Exception as e:
            logger.warning(f"Failed to lemmatize Bulgarian text: {e}")
            return text
    
    def _normalize_aliases(self, query: str) -> str:
        """
        BUSINESS_RULE: Normalize query using canonical aliases with light Bulgarian stemming.
        
        Args:
            query: Original user query
            
        Returns:
            Normalized query with canonical forms
        """
        query_lower = query.lower()
        
        # Light Bulgarian stemming - remove common endings
        stemmed_query = self._light_stem_bulgarian(query_lower)
        
        # Check for exact alias matches (case-insensitive)
        original_query = query
        for alias, canonical in self.aliases.items():
            if alias in query_lower or alias in stemmed_query:
                # Replace the alias with canonical form (case-insensitive)
                import re
                query = re.sub(re.escape(alias), canonical, query, flags=re.IGNORECASE)
                logger.info(f"Normalized alias: '{alias}' -> '{canonical}' in query: '{query}'")
                break
        
        # Morphological fallback - –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞ –≥–ª–∞–≥–æ–ª–Ω–∏ —Ñ–æ—Ä–º–∏
        if query == original_query:  # –ê–∫–æ –Ω–µ –µ –Ω–∞–º–µ—Ä–µ–Ω alias
            for base_word in ["–ø–ª–∞—â–∞", "–¥–æ—Å—Ç–∞–≤–∫–∞", "–≤—Ä—ä—â–∞", "–≥–∞—Ä–∞–Ω—Ü–∏—è", "–ø—Ä–æ–º–æ", "–∫–æ–¥", "–∞–¥—Ä–µ—Å"]:
                if base_word in query_lower:
                    for alias, canonical in self.aliases.items():
                        if base_word in alias:
                            import re
                            query = re.sub(re.escape(base_word), canonical, query, flags=re.IGNORECASE)
                            logger.info(f"Morphological fallback normalized '{base_word}' -> '{canonical}' in query: '{query}'")
                            break
                    break
        
        # Double-pass semantic check - –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç—è–≤–∞ –≥—Ä–µ—à–Ω–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        if query != original_query:
            try:
                from sentence_transformers import util
                norm_emb = self.embedding_model.encode(query)
                orig_emb = self.embedding_model.encode(original_query)
                similarity = util.cos_sim(norm_emb, orig_emb).item()
                
                if similarity < 0.6:
                    logger.debug(f"Alias normalization changed semantics too much ({similarity:.2f}), reverting")
                    query = original_query
                else:
                    logger.debug(f"Alias normalization preserved semantics ({similarity:.2f})")
            except Exception as e:
                logger.warning(f"Semantic check failed: {e}, keeping normalized query")
        
        return query
    
    def _light_stem_bulgarian(self, text: str) -> str:
        """
        Light Bulgarian stemming - remove common endings for better matching.
        
        Args:
            text: Text to stem
            
        Returns:
            Stemmed text
        """
        # Common Bulgarian endings to remove
        endings = ['-—Ç–∞', '-—Ç–æ', '-—Ç–µ', '-–∏—Ç–µ', '-–∞—Ç–∞', '-–æ—Ç–æ', '-–µ—Ç–æ', '-–∏—è', '-–∏—è—Ç–∞']
        
        stemmed = text
        for ending in endings:
            if stemmed.endswith(ending):
                stemmed = stemmed[:-len(ending)]
                break
        
        return stemmed
    
    def recognize_intent(self, query: str) -> Tuple[str, float, str]:
        """
        BUSINESS_RULE: Recognize user intent using semantic similarity.
        
        Args:
            query: User's original query
            
        Returns:
            Tuple of (intent_category, confidence_score, rewritten_query)
        """
        logger.debug(f"Recognizing intent for: '{query}'")
        
        # Encode the input query
        query_embedding = self.embedding_model.encode(query)
        
        best_intent = "general"
        best_score = 0.0
        best_canonical = query
        
        # Compare with precomputed canonical embeddings (optimized)
        for intent, example_embeddings in self.intent_embeddings.items():
            similarities = util.cos_sim(query_embedding, example_embeddings)
            max_score_idx = np.argmax(similarities)
            max_score = similarities[0, max_score_idx].item()
            
            # Debug: Log similarity for each intent
            logger.debug(f"Intent '{intent}': max similarity = {max_score:.3f}")
            
            if max_score > best_score:
                best_score = max_score
                best_intent = intent
                best_canonical = self.canonical_queries[intent][max_score_idx]
        
        logger.debug(f"Best intent: '{best_intent}' with score {best_score:.3f}")
        
        # Only rewrite if confidence is high enough (further lowered for Bulgarian)
        if best_score > 0.45:  # Further lowered threshold for Bulgarian models
            logger.info(f"Intent recognized: {best_intent} (confidence: {best_score:.2f})")
            return best_intent, best_score, best_canonical
        else:
            logger.debug(f"No clear intent found, using original query")
            return "general", best_score, query
    
    def expand_query(self, query: str) -> str:
        """
        BUSINESS_RULE: Expand query with semantic synonyms for better retrieval.
        
        Args:
            query: Original query
            
        Returns:
            Expanded query with synonyms
        """
        expanded_terms = []
        query_lower = query.lower()
        
        # Add original query
        expanded_terms.append(query)
        
        # Add synonyms for key terms with dynamic weighting
        for term, synonyms in self.synonyms.items():
            if term in query_lower:
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞ –∫–ª—é—á–æ–≤–∏ —Ç–µ—Ä–º–∏–Ω–∏ - —Å–∞–º–æ –Ω–∞–π-—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ç–µ
                priority_synonyms = synonyms[:3]  # —Å–∞–º–æ –ø—ä—Ä–≤–∏—Ç–µ 3
                for synonym in priority_synonyms:
                    if synonym not in query_lower:
                        expanded_terms.append(synonym)
        
        # Create expanded query
        if len(expanded_terms) > 1:
            expanded_query = f"{query} {' '.join(expanded_terms[1:])}"
            logger.debug(f"Query expanded: '{query}' -> '{expanded_query}'")
            return expanded_query
        
        return query
    
    def _check_product_exists_in_db(self, product_type: str) -> bool:
        """
        BUSINESS_RULE: Check if the extracted product type actually exists in database.
        
        Args:
            product_type: The product type extracted from user query
            
        Returns:
            True if product exists in database, False otherwise
        """
        try:
            from src.database.db import get_db_connection
            
            conn = get_db_connection()
            cursor = conn.cursor()
            
            # Check if any products match the extracted product type
            sql = """
                SELECT COUNT(*) 
                FROM products 
                WHERE LOWER(name) LIKE %s 
                   OR LOWER(description) LIKE %s 
                   OR LOWER(category) LIKE %s
            """
            
            product_lower = product_type.lower()
            cursor.execute(sql, (f'%{product_lower}%', f'%{product_lower}%', f'%{product_lower}%'))
            count = cursor.fetchone()[0]
            
            cursor.close()
            conn.close()
            
            exists = count > 0
            logger.info(f"Product '{product_type}' exists in DB: {exists} (found {count} matches)")
            return exists
            
        except Exception as e:
            logger.error(f"Error checking product existence: {e}")
            return False
    
    def add_context_boost(self, query: str, intent: str) -> str:
        """
        BUSINESS_RULE: –î–æ–±–∞–≤—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ –ø–æ–¥—Å–∏–ª–≤–∞–Ω–µ –∫—ä–º –∑–∞—è–≤–∫–∞—Ç–∞.
        
        Args:
            query: –û—Ä–∏–≥–∏–Ω–∞–ª–Ω–∞ –∑–∞—è–≤–∫–∞
            intent: –†–∞–∑–ø–æ–∑–Ω–∞—Ç–æ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ
            
        Returns:
            –ó–∞—è–≤–∫–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ –ø–æ–¥—Å–∏–ª–≤–∞–Ω–µ
        """
        if intent in self.context_boosters:
            context_terms = self.context_boosters[intent]
            boosted_query = f"{query} {context_terms}"
            logger.debug(f"Context boosted: '{query}' -> '{boosted_query}' (intent: {intent})")
            return boosted_query
        
        return query
    
    def process_query(self, query: str) -> Dict[str, any]:
        """
        BUSINESS_RULE: Complete query processing with intent recognition and expansion.
        
        Args:
            query: User's original query
            
        Returns:
            Dictionary with processed query information
        """
        original_query = query.strip()
        query_lower = original_query.lower()
        
        # FAST-PATH FIRST: Check for products before any processing
        # This ensures we catch product queries before lemmatization changes the text
        products_match = re.search(
            r"\b(?:–ø—Ä–µ–¥–ª–∞–≥–∞—Ç–µ –ª–∏|–∏–º–∞—Ç–µ –ª–∏|–ø—Ä–æ–¥–∞–≤–∞—Ç–µ –ª–∏|–Ω–∞–ª–∏—á–Ω–∏ –ª–∏ —Å–∞|–∏–º–∞ –ª–∏|–∫—É–ø—è|–ø–æ—Ä—ä—á–∞–º|–≤–∑–µ–º–µ—Ç–µ –ª–∏)\s+([a-zA-Z–∞-—è–ê-–Ø0-9\s-]+?)\b(?:\?|$)",
            query_lower
        )
        if products_match:
            product_type = products_match.group(1).strip()
            # Only classify as product if the word after the verb is a real product
            if any(kw in product_type.lower() for kw in [
                "–ª–∞–ø—Ç–æ–ø", "–Ω–æ—É—Ç–±—É–∫", "–∫–æ–º–ø—é—Ç—ä—Ä", "—Ç–µ–ª–µ—Ñ–æ–Ω", "—Å–º–∞—Ä—Ç—Ñ–æ–Ω", "—Å–º–∞—Ä—Ç—Ñ–æ–Ω–∏", "gsm", 
                "—á–∞—Å–æ–≤–Ω–∏–∫", "—Ç–∞–±–ª–µ—Ç", "—Å–ª—É—à–∞–ª–∫–∏", "–º–∏—à–∫–∞", "–∫–ª–∞–≤–∏–∞—Ç—É—Ä–∞", "–º–æ–Ω–∏—Ç–æ—Ä",
                "–ø—Ä–∏–Ω—Ç–µ—Ä", "—Å–∫–µ–Ω–µ—Ä", "–∫–∞–º–µ—Ä–∞", "–∞–∫—Å–µ—Å–æ–∞—Ä–∏", "–ø–µ—Ä–∏—Ñ–µ—Ä–∏—è", "—Å–º–∞—Ä—Ç—Ñ–æ–Ω",
                "iphone", "samsung", "dell", "hp", "lenovo", "asus", "acer"
            ]):
                result = {
                    "original_query": original_query,
                    "processed_query": original_query,
                    "intent": "products",
                    "confidence": 0.95,
                    "query_type": "regex_fastpath",
                    "product_filter": product_type,
                    "expanded_terms": []
                }
                logger.info(f"‚úì Regex fast-path (product='{product_type}') for '{original_query}'")
                return result
        
        # Fallback regex for general products queries
        if re.search(r"\b(–∫–∞–∫–≤–∏ –ø—Ä–æ–¥—É–∫—Ç–∏( –∏–º–∞—Ç–µ)?|–∫–∞—Ç–∞–ª–æ–≥( –ø—Ä–æ–¥—É–∫—Ç–∏)?|–∫–∞—Ç–∞–ª–æ–≥–∞|–ø—Ä–æ–¥—É–∫—Ç–∏|—Å—Ç–æ–∫–∏)\b", query_lower):
            result = {
                "original_query": original_query,
                "processed_query": original_query,
                "intent": "products",
                "confidence": 0.95,
                "query_type": "regex_fastpath",
                "product_filter": None,
                "expanded_terms": []
            }
            logger.info(f"‚úì Regex fast-path (general products) for '{original_query}'")
            return result
        
        # FAST-PATH: Working hours queries (days of week)
        if re.search(r"\b(—Ä–∞–±–æ—Ç–∏—Ç–µ –ª–∏|–æ—Ç–≤–æ—Ä–µ–Ω–∏ –ª–∏ —Å—Ç–µ|—Ä–∞–±–æ—Ç–Ω–æ –≤—Ä–µ–º–µ|–≥—Ä–∞—Ñ–∏–∫|–∫–æ–≥–∞ —Ä–∞–±–æ—Ç–∏—Ç–µ|–¥–æ –∫–æ–ª–∫–æ —á–∞—Å–∞|–æ—Ç –∫–æ–ª–∫–æ —á–∞—Å–∞)\b", query_lower):
            # Check for specific days
            days_match = re.search(r"\b(–ø–æ–Ω–µ–¥–µ–ª–Ω–∏–∫|–≤—Ç–æ—Ä–Ω–∏–∫|—Å—Ä—è–¥–∞|—á–µ—Ç–≤—ä—Ä—Ç—ä–∫|–ø–µ—Ç—ä–∫|—Å—ä–±–æ—Ç–∞|–Ω–µ–¥–µ–ª—è|–ø–æ—á–∏–≤–Ω–∏ –¥–Ω–∏|–ø—Ä–∞–∑–Ω–∏—Ü–∏)\b", query_lower)
            result = {
                "original_query": original_query,
                "processed_query": original_query,
                "intent": "working_hours",
                "confidence": 0.95,
                "query_type": "regex_fastpath",
                "day_filter": days_match.group(1) if days_match else None,
                "expanded_terms": []
            }
            logger.info(f"‚úì Regex fast-path (working hours) for '{original_query}'")
            return result
        
        # Step -1: Auto-correct Bulgarian spelling before regex/intent analysis
        query_lower = self._auto_correct_bulgarian(query_lower)
        
        # Step 0: Lemmatize Bulgarian words to base forms
        query_lower = self._lemmatize_bulgarian(query_lower)

        # Step 0: Normalize query with aliases
        normalized_query = self._normalize_aliases(original_query)

        # Step 1: Recognize intent (primary pass)
        intent, confidence, canonical_query = self.recognize_intent(normalized_query)

        # Step 2: Retry with original query if low confidence
        if intent == "general" and normalized_query != original_query:
            intent, confidence, canonical_query = self.recognize_intent(original_query)

        # Step 3: Fallback to keyword-based intent if still uncertain
        if confidence < 0.45:
            keyword_intents = {
                "–ø–ª–∞—â–∞": "payment",
                "–ø–ª–∞—â–∞–Ω–µ": "payment", 
                "–ø–ª–∞—â–∞–º": "payment",
                "—Ä–∞–∑–ø–ª–∞—â–∞–Ω–µ": "payment",
                "–¥–æ—Å—Ç–∞–≤–∫–∞": "delivery",
                "–¥–æ—Å—Ç–∞–≤—è–Ω–µ": "delivery",
                "–≤—Ä—ä—â–∞–Ω–µ": "return_policy",
                "–≤—ä—Ä–Ω–∞": "return_policy",
                "—Ä–µ–∫–ª–∞–º–∞—Ü–∏—è": "return_policy",
                "–≥–∞—Ä–∞–Ω—Ü–∏—è": "warranty",
                "—Å–µ—Ä–≤–∏–∑": "warranty",
                "–∫–æ–Ω—Ç–∞–∫—Ç": "contact",
                "—Ç–µ–ª–µ—Ñ–æ–Ω": "contact",
                "–ø—Ä–æ–¥—É–∫—Ç": "products",
                "–ø—Ä–æ–¥—É–∫—Ç–∏": "products",
                "—Å—Ç–æ–∫–∏": "products",
                "–ø—Ä–æ–¥–∞–≤–∞—Ç–µ": "products",
                "–∞—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç": "products",
                "–∫–∞—Ç–∞–ª–æ–≥": "products",
                "–ø—Ä–æ–º–æ": "promotions"
            }
            for kw, kw_intent in keyword_intents.items():
                if kw in query_lower:
                    logger.debug(f"Keyword-based intent match: {kw} ‚Üí {kw_intent}")
                    intent = kw_intent
                    confidence = 0.55
                    canonical_query = self.canonical_queries[kw_intent][0] if kw_intent in self.canonical_queries else original_query
                    break

        # Step 4: Expand query with synonyms
        expanded_query = self.expand_query(normalized_query)
        
        # Step 5: Add context boost based on intent
        if intent != "general":
            expanded_query = self.add_context_boost(expanded_query, intent)

        # Step 6: Decide final form
        if confidence > 0.7:
            final_query = canonical_query
            query_type = "canonical"
        elif len(expanded_query) > len(original_query):
            final_query = expanded_query
            query_type = "expanded"
        else:
            final_query = normalized_query
            query_type = "normalized"
        
        result = {
            "original_query": original_query,
            "processed_query": final_query,
            "intent": intent,
            "confidence": round(confidence, 3),
            "query_type": query_type,
            "expanded_terms": expanded_query.split() if expanded_query != original_query else []
        }
        
        logger.info(f"üß† Query processed: {query_type.upper()} (intent: {intent}, conf: {confidence:.2f}) -> '{final_query}'")
        return result
    
    def get_intent_examples(self, intent: str) -> List[str]:
        """
        Get example queries for a specific intent.
        
        Args:
            intent: Intent category
            
        Returns:
            List of example queries
        """
        return self.canonical_queries.get(intent, [])
    
    def get_all_intents(self) -> List[str]:
        """
        Get all available intent categories.
        
        Returns:
            List of intent categories
        """
        return list(self.canonical_queries.keys())
